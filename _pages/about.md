---
permalink: /
title: "Xueqing Deng's personal webpage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm currently a research scientist at [ByteDance Seed Research](https://seed.bytedance.com/en/) and I joined ByteDance since 2022. During ByteDance, I am fortunate to work under the supervision from [Dr. Liang-Chieh Chen](https://scholar.google.com/citations?user=ACjYGPUAAAAJ&hl=en) and [Dr. Xiaohui Shen](https://scholar.google.com/citations?user=pViZYwIAAAAJ&hl=en).  Before joining ByteDance, I achieved my PhD degree in EECS from University of California, Merced in 2021 advised by [Prof. Shawn Newsam](https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=pQZX0mEAAAAJ&sortby=pubdate) and worked closely with [Dr. Yi Zhu](https://scholar.google.com/citations?user=IXw4UiwAAAAJ&hl=en) during my PhD. Before that, I achieved my BS dregree in 2016 from Sun Yat-Sen University, China.

You can find my resume here: [Xueqing Deng's Resume](https://xdeng7.github.io/xqdeng77.github.io/assets/cv_xueqing.pdf)

[Google Scholar](https://scholar.google.com/citations?user=UGhyv2UAAAAJ&hl=en)/[Github](https://github.com/xdeng7)

Research Interests
------
I am recently interested in multimodal learning for visual generation including both image and video.

News!
------
6/10/2025: 2 ICCV2025 have been accepted!

2/24/2025: 1 CPVR2025 has been accepted!

10/1/2024: 1 NeurIPS2024 has been accepted!

Selected Work
------
<table border="0" style="border-collapse: collapse;">
   <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/videoweaver.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception
      </a><br>
      Zhiheng Liu*, <b><u>Xueqing Deng</u></b>, Shoufa Chen, Angtian Wang, Qiushan Guo, Mingfei Han, Zeyue Xue, Mengzhao Chen, Ping Luo, Linjie Yang (*denotes mentored intern)<br>
      <em>coming, 2025</em><br>
      <a href="https://your-paper-link.com">coming</a> /
      <a href="https://your-project-link.com">coming</a>
    </td>
  </tr>
  
  <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/psgbench.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        Leveraging Panoptic Scene Graph for Evaluating Fine-Grained Text-to-Image Generation
      </a><br>
      <b><u>Xueqing Deng</u></b>, Linjie Yang, Qihang Yu, Chenglin Yang, Liang-Chieh Chen<br>
      <em>ICCV, 2025</em><br>
      <a href="https://your-paper-link.com">coming</a> /
      <a href="https://your-project-link.com">coming</a>
    </td>
  </tr>
  
  <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/vicas.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation
      </a><br>
      Ali Athar, <b><u>Xueqing Deng</u></b>, Liang-Chieh Chen<br>
      <em>CVPR, 2025</em><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Athar_ViCaS_A_Dataset_for_Combining_Holistic_and_Pixel-level_Video_Understanding_CVPR_2025_paper.html">paper</a> /
      <a href="https://ali2500.github.io/vicas-project/">webpage</a>
    </td>
  </tr>
    <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/coconut_pancap.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation
      </a><br>
     <b><u>Xueqing Deng</u></b>, Linjie Yang, Qihang Yu, Ali Athar, Chenglin Yang, Xiaojie Jin, Xiaohui Shen, Liang-Chieh Chen<br>  
      <em>arxiv, 2025</em><br>
      <a href="https://arxiv.org/abs/2502.02589">paper</a> /
      <a href="https://xdeng7.github.io/coconut.github.io/coconut_pancap.html">webpage</a>
    </td>
  </tr>
      <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/coconut.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        COCONut: Modernizing COCO Segmentation
      </a><br>
     <b><u>Xueqing Deng</u></b>, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen<br>  
      <em>CVPR, 2024</em><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Deng_COCONut_Modernizing_COCO_Segmentation_CVPR_2024_paper.pdf">paper</a> /
      <a href="https://xdeng7.github.io/coconut.github.io/">webpage</a>
    </td>
  </tr>
       <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/distpro.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization
      </a><br>
     <b><u>Xueqing Deng*</u></b>, Dawei Sun*, Shawn Newsam and Peng Wang (*denotes co-first author)<br>  
      <em>ECCV, 2022</em><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_NightLab_A_Dual-Level_Architecture_With_Hardness_Detection_for_Segmentation_at_CVPR_2022_paper.html">paper</a> 
    </td>
  </tr>
   <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/nightlab.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        NightLab: A Dual-Level Architecture With Hardness Detection for Segmentation at Night
      </a><br>
     <b><u>Xueqing Deng</u></b>, Peng Wang, Xiaochen Lian, Shawn Newsam<br>  
      <em>CVPR, 2022</em><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_NightLab_A_Dual-Level_Architecture_With_Hardness_Detection_for_Segmentation_at_CVPR_2022_paper.html">paper</a> 
    </td>
  </tr>
     <tr>
    <td style="padding-right:10px; vertical-align:top;">
      <img src="https://xdeng7.github.io/xqdeng77.github.io/assets/autoadapt.png" alt="缩略图" width="250">
    </td>
    <td style="vertical-align:top;">
      <a href="https://your-paper-link.com" style="font-size:16px; font-weight:bold; text-decoration:none;">
        AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain Adaptation
      </a><br>
     <b><u>Xueqing Deng</u></b>, Yi Zhu, Yuxin Tian, Shawn Newsam<br>  
      <em>CVPR Workshop on Neural Architecture Search, 2021</em><br>
      <a href="https://arxiv.org/abs/2106.13227">paper</a> 
    </td>
  </tr>

</table>


